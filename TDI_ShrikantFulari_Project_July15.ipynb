{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\shriks44\\Anaconda3_2019\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "## Import the required libraries\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shriks44\\Anaconda3_2019\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "## Read the data set here\n",
    "\n",
    "alldf = pd.read_csv(\"E:\\\\Data Science\\\\The_Data_Incubator\\\\Motor_Vehicle_Collisions_-_Crashes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detailed_project_summary = \"\"\" Motivation:\n",
    "\n",
    "1. Vehicle crashes occur all over the world, with varying severity, traffic rules, vehicle characteristics, driving behavior, weather etc., each of which play a significant role in the outcome of the crash. \n",
    "\n",
    "2. While most of the data science applications for crash analysis have been only in advanced visualization, there is limited study of developing advanced machine learning algorithms that can help predict future crashes and its severities based on several parameters.\n",
    "\n",
    "3. The limited existing models are developed for much larger areas which may result in overfit/underfit issue. Hence, 'One model fits all' is an outdated strategy and will not be beneficial in providing actionable insights to zone specific crashes which may have different parameters. \n",
    "\n",
    "Project at The Data Incubator:\n",
    "\n",
    "1. Through my project at the Data Incubator, I wish to develop an automated system which in return develops machine learning models to predict future fatal or non fatal crashes for any chosen area. \n",
    "\n",
    "2. These models would be focussed towards very specific zones/areas/road corridors, which I term as micro-crash models.\n",
    "\n",
    "3. The intention here is to develop custom models that are built solely on the data that is represented by that zone/area rather than a much larger set. \n",
    "\n",
    "4. Having such area specific models through an automated system will be beneficial to a lot of state or federal Department's of Transport who can have access to a tool that can deliver key actionable insights, and help direct their crash preventive measures more effectively in future.\n",
    "\n",
    "5. This can help direct their budget, initiatives, and efforts to specific locations that need the most attention. Outcomes of such models can be imposing traffic regulations during peak crash hours, zones, restricitng certain category of vehicles etc.\n",
    "\n",
    "Data availability:\n",
    "\n",
    "1. A lot of Transportation agencies, City DOT's, Federal DOT's provide traffic and crash related open data that can be easily acquired to develop this bigger idea. \n",
    "\n",
    "2. I wish to develop a full fledged application that can be responsive in real time and update the models with live feed of additional data through pipelines whenever available. As of today, there is no such automated model development application in Transportation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Welcome to New York City Micro-Crash modeling\")\n",
    "root.geometry(\"1000x600\")\n",
    "\n",
    "#alldf = pd.read_csv(\"E:\\\\Data Science\\\\The_Data_Incubator\\\\Motor_Vehicle_Collisions_-_Crashes.csv\")\n",
    "\n",
    "option_1 = ['Select', 'ZIP CODE', 'BOROUGH']\n",
    "\n",
    "option_2 = list(alldf['BOROUGH'].unique())\n",
    "option_2 = [x for x in option_2 if str(x) != 'nan']\n",
    "option_2.insert(0, 'Select')\n",
    "\n",
    "\n",
    "#################### user_entry_frame ###########################\n",
    "user_entry_frame = tk.Frame(root, background=\"light blue\", borderwidth = 1,highlightthickness=3)\n",
    "user_entry_frame.place(x=0, y=0, anchor=\"nw\", width=450, height=300)\n",
    "\n",
    "user_label_1 = Label(user_entry_frame, text=\"Model development inputs\", bg=\"sky blue\")\n",
    "user_label_1.grid(row=0, column=0, padx=1, pady= 1)\n",
    "\n",
    "user_label_2 = Label(user_entry_frame, text=\"Select if you would like a model based on Zip Codes or Borough\")\n",
    "user_label_2.grid(row=2, column=0, padx= 15, pady= 10)\n",
    "\n",
    "myCombo1 = ttk.Combobox(user_entry_frame, value=option_1)\n",
    "myCombo1.current(0)\n",
    "#myCombo1.bind(\"<<ComboboxSelected>>\", selected)\n",
    "myCombo1.grid(row=4, column=0, padx = 0, pady= 10)\n",
    "\n",
    "\n",
    "\n",
    "#### Model selection\n",
    "\n",
    "def model_prediction():\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    criteria_1 = myCombo1.get()\n",
    "    zone_id = myCombo2.get()\n",
    "    df = alldf[alldf[criteria_1] == zone_id]\n",
    "    \n",
    "    ## We will try to model total fatalities that occur for a particular incident\n",
    "    ## In our modelling case, we would like to have strong predictors. We can obtain few indicators from the date column\n",
    "    ## We can split the date column and obtain crucial information such as month and weekday/weekend which may provide crucial insights\n",
    "    df['month'] = pd.DatetimeIndex(df['CRASH DATE']).month\n",
    "    df['pd_datetime'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    df['weekday'] = df['pd_datetime'].dt.dayofweek\n",
    "    df['hour'] = pd.to_datetime(df['CRASH TIME']).dt.hour\n",
    "    \n",
    "    ## We can see that number of fatalities is a continuous random variable. Hence we will categorize that data into fatal(1) or non-fatal(0)\n",
    "    df['output'] = df['NUMBER OF PERSONS KILLED'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "    ## Similarly, we will make other predictors as categorical.\n",
    "    ## Based on initial screening, will use our sample predictors as weekday, hour, month, 'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
    "    #'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5',\n",
    "    ## We will convert the non-numeric data into numeric by usink a lebel-encoder\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le_factor = LabelEncoder()\n",
    "    uniquelist_factor = df['CONTRIBUTING FACTOR VEHICLE 1'].unique().tolist() + df['CONTRIBUTING FACTOR VEHICLE 2'].unique().tolist() + df['CONTRIBUTING FACTOR VEHICLE 3'].unique().tolist() + df['CONTRIBUTING FACTOR VEHICLE 4'].unique().tolist() + df['CONTRIBUTING FACTOR VEHICLE 5'].unique().tolist()\n",
    "    uniquelist_factor = list(dict.fromkeys(uniquelist_factor))\n",
    "    le_factor.fit(uniquelist_factor)\n",
    "    df['CONTRIBUTING FACTOR VEHICLE 1'] = le_factor.transform(df['CONTRIBUTING FACTOR VEHICLE 1'].astype(str))\n",
    "    df['CONTRIBUTING FACTOR VEHICLE 2'] = le_factor.transform(df['CONTRIBUTING FACTOR VEHICLE 2'].astype(str))\n",
    "    df['CONTRIBUTING FACTOR VEHICLE 3'] = le_factor.transform(df['CONTRIBUTING FACTOR VEHICLE 3'].astype(str))\n",
    "    df['CONTRIBUTING FACTOR VEHICLE 4'] = le_factor.transform(df['CONTRIBUTING FACTOR VEHICLE 4'].astype(str))\n",
    "    df['CONTRIBUTING FACTOR VEHICLE 5'] = le_factor.transform(df['CONTRIBUTING FACTOR VEHICLE 5'].astype(str))\n",
    "    \n",
    "    \n",
    "    # We can see that there are only a few fatal cases as compared to non-fatal cases.\n",
    "    # Hence we will select the data to have equal values of both\n",
    "    least_samples = df.groupby('output')['hour'].count().min()\n",
    "    nonfatal_df = df[df['output'] == 0].sample(n=least_samples)\n",
    "    fatal_df = df[df['output'] == 1]\n",
    "    combined = [nonfatal_df, fatal_df]\n",
    "    final_output_df = pd.concat(combined)\n",
    "    \n",
    "    \n",
    "    ## Now we will define our input and output variables for developing the model\n",
    "    trainset = final_output_df[['weekday','month','hour','CONTRIBUTING FACTOR VEHICLE 1','CONTRIBUTING FACTOR VEHICLE 2','CONTRIBUTING FACTOR VEHICLE 3','CONTRIBUTING FACTOR VEHICLE 4','CONTRIBUTING FACTOR VEHICLE 5']]\n",
    "    labels  = final_output_df['output']\n",
    "    \n",
    "    ## To enhance reliability of the accuracy score, we will use different folds of the data for training and testing\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(10, test_size = 0.25, random_state = 23)\n",
    "\n",
    "    sss.get_n_splits(trainset, labels)\n",
    "    train_x, test_x, train_y, test_y = [],[],[],[]\n",
    "\n",
    "    for train_index, test_index in sss.split(trainset, labels):\n",
    "        train_x.append(trainset.iloc[train_index])\n",
    "        test_x.append(trainset.iloc[test_index])\n",
    "        train_y.append(labels.iloc[train_index])\n",
    "        test_y.append(labels.iloc[test_index])\n",
    "        \n",
    "    ### We will use 7 diffeent classifiers to test the model accuracy\n",
    "     \n",
    "    from sklearn import metrics as sm\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel='rbf', C=0.025, probability=True, gamma='scale'),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        GaussianNB(),\n",
    "        LogisticRegression(),\n",
    "        AdaBoostClassifier(n_estimators = 30)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    def Average(lst): \n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "    from prettytable import PrettyTable\n",
    "    Results_table = PrettyTable(['Classifier', 'Accuracy'])\n",
    "\n",
    "    classifier_name = []\n",
    "    classifier_accuracy = []\n",
    "\n",
    "    for clf in classifiers:\n",
    "        acc_total=[]\n",
    "        for i in range(len(train_x)):\n",
    "        \n",
    "            clf.fit(train_x[i], train_y[i])\n",
    "            #name = type(clf).__name__\n",
    "            model_name = clf.__class__.__name__\n",
    "\n",
    "            train_predictions = clf.predict(test_x[i])\n",
    "            acc = sm.accuracy_score(test_y[i] , train_predictions)\n",
    "            acc_total.append(acc)\n",
    "        Results_table.add_row([str(model_name), \"{:.2%}\".format(Average(acc_total))])\n",
    "        classifier_name.append(str(model_name))\n",
    "        classifier_accuracy.append(Average(acc_total)*100)\n",
    "    \n",
    "    ### Here we will develop a plot to show summary of classifier accuracy\n",
    "    \n",
    "    fig_model_output = Figure(figsize=(5.4,2.9), dpi=100)\n",
    "    ax1 = fig_model_output.add_subplot(111)\n",
    "    ax1.barh(classifier_name,classifier_accuracy)\n",
    "    ax1.set_xlabel('Accuracy (%)')\n",
    "    ax1.set_ylabel('Classifier') \n",
    "    ax1.set_title('Model summary for: ' + zone_id)\n",
    "    ax1.set_xticks(np.arange(0,100,10))\n",
    "    ax1.xaxis.set_tick_params(labelsize=8)\n",
    "    ax1.yaxis.set_tick_params(labelsize=8)\n",
    "    fig_model_output.tight_layout()\n",
    "    \n",
    "    canvas_model_output = FigureCanvasTkAgg(fig_model_output, master=model_output_frame)\n",
    "    canvas_model_output.get_tk_widget().grid(row=0, column=0)\n",
    "    canvas_model_output.draw()\n",
    "    \n",
    "    ### Here we will develop a plot to show fatalities by hour for the selected zone\n",
    "    \n",
    "    chart_data = df.groupby('hour')['NUMBER OF PERSONS KILLED'].sum().reset_index()\n",
    "    fig_statistics = Figure(figsize=(5.4,2.9), dpi=100)\n",
    "    ax2 = fig_statistics.add_subplot(111)\n",
    "    ax2.bar(chart_data['hour'], chart_data['NUMBER OF PERSONS KILLED'], color='g')\n",
    "    ax2.set_xlabel('Hour')\n",
    "    ax2.set_ylabel('Total fatalities') \n",
    "    ax2.set_title('Total fatalities by hour for : ' + zone_id)\n",
    "    ax2.set_xticks(np.arange(len(chart_data['hour'])))\n",
    "    ax2.xaxis.set_tick_params(labelsize=8)\n",
    "    ax2.yaxis.set_tick_params(labelsize=8)\n",
    "    fig_statistics.tight_layout()\n",
    "\n",
    "    canvas_statistics = FigureCanvasTkAgg(fig_statistics, master=zone_summary_frame)\n",
    "    canvas_statistics.get_tk_widget().grid(row=0, column=0)\n",
    "    canvas_statistics.draw()\n",
    "\n",
    "    \n",
    "### Model selection done\n",
    "    \n",
    "l2 = Label(user_entry_frame, text=\"Select the corresponding Zip Code or Borough\")\n",
    "l2.grid(row=6, column=0, padx=0, pady= 10)\n",
    "\n",
    "\n",
    "myCombo2 = ttk.Combobox(user_entry_frame, value = option_2)\n",
    "myCombo2.current(0)\n",
    "#myCombo2.bind(\"<<ComboboxSelected>>\", model_prediction)\n",
    "myCombo2.grid(row=8, column=0, padx=0, pady= 10)\n",
    "\n",
    "generate_model_button = tk.Button(user_entry_frame, text=\"Click here to generate model and results\", command = model_prediction)\n",
    "generate_model_button.grid(row=10, column=0, padx=10, pady= 40)\n",
    "\n",
    "\n",
    "#################### project_summary_frame ###########################\n",
    "project_summary_frame = tk.Frame(root, background=\"light blue\", borderwidth = 1,highlightthickness=3)\n",
    "project_summary_frame.place(x=0, y=300, anchor=\"nw\", width=450, height=300)\n",
    "\n",
    "\n",
    "l1 = Label(project_summary_frame, bg=\"sky blue\")\n",
    "l1.grid(row=0, column=0, padx=1, pady= 1)\n",
    "\n",
    "\n",
    "def show_project_summary ():\n",
    "    global shown\n",
    "    if shown: project_summary.grid_remove () # Hide the text\n",
    "    else: project_summary.grid(row=2, column=0, padx=2, pady=5) # Show the text\n",
    "    shown = not shown # Reverse the 'shown' boolean value\n",
    "    \n",
    "    \n",
    "project_summary = Text(project_summary_frame, height=15, width=54, wrap= WORD)#, yscrollcommand = scrollbar.set)\n",
    "#project_summary.grid(row=1, column=0)\n",
    "project_summary.insert(INSERT, Detailed_project_summary)\n",
    "\n",
    "shown = False\n",
    "generate_summary_button = tk.Button(project_summary_frame, text=\"Kindly click here for project summary\", command = show_project_summary, bg=\"sky blue\")\n",
    "generate_summary_button.grid(row=0, column=0, padx = 0, pady = 2)\n",
    "                           \n",
    "\n",
    "#################### model_output_frame ###########################\n",
    "model_output_frame = tk.Frame(root, background=\"light blue\", borderwidth = 1,highlightthickness=3)\n",
    "model_output_frame.place(x=450, y=0, anchor=\"nw\", width=550, height=300)\n",
    "\n",
    "\n",
    "l1 = Label(model_output_frame, text=\"Model performance charts\", bg=\"sky blue\")\n",
    "l1.grid(row=0, column=0, padx=1, pady= 1)\n",
    "\n",
    "#chart = FigureCanvasTkAgg(fig, model_output_frame)\n",
    "#chart.get_tk_widget().grid(row=0, column=0, padx=1, pady= 1)\n",
    "\n",
    "#################### zone_summary_frame ###########################\n",
    "zone_summary_frame = tk.Frame(root, background=\"light blue\", borderwidth = 1,highlightthickness=3)\n",
    "zone_summary_frame.place(x=450, y=300, anchor=\"nw\", width=550, height=300)\n",
    "\n",
    "l1 = Label(zone_summary_frame, text=\"Zone summary charts\", bg=\"sky blue\")\n",
    "l1.grid(row=0, column=0, padx=1, pady= 1)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
